# Inference_withTorchTensorRT
Comparing Deep Learning Inference of Pytorch models running on CPU, CUDA and TensorRT
<br>
<br>
![Thumbnail_Torch-TensorRT9](https://user-images.githubusercontent.com/32107652/154847750-b6641e53-f2e9-4646-aa6d-b5c91c86ff08.png)
<h2>YouTube Tutorial</h2>
https://youtu.be/iFADsRDJhDM
<br>
<h2>Please note</h2>
this .ipynb notebook is meant to run in a Torch TensorRT Docker container
<br>
or an Nvidia NGC container.
<br>
You can find detailed setup instructions is the video above.
<br>
<h2>About</h2>
<b>author:</b> Mariya Sha
<br>
<b>dependencies: </b> Pytorch, Torchvision, Panadas, TorchTensorRT
<br>
<br>
In this notebook we will run inference with: CPU, CUDA and TensorRT and
<br>
compair their speed with a special benchmarking utility function.
<br>
We will load a pre-trained Neural Netowork (<b>ResNet50</b>) and we will
<br>
use it to predict a never-before-seen picture of my cat.
<br>
you can either use my picture (<b>img1.jpg</b>) or you can choose one
<br>
from your personal gallery (highly reccomended!)
<br>
<br>
please feel free to use my code anywhere you'd like, no need to credit me!
<br>
howerver - if you do, I'll really appreciate it :)
